{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3c04dab",
      "metadata": {
        "id": "c3c04dab"
      },
      "source": [
        "# Deep Learning Foundations Assignment\n",
        "**Name:**\n",
        "**Date:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27dde26a",
      "metadata": {
        "id": "27dde26a"
      },
      "source": [
        "## Part 1 — Neural Networks (Tabular FFNN)\n",
        "### 1.1 Single Neuron Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12874447",
      "metadata": {
        "id": "12874447",
        "outputId": "d8cac654-b3dd-4f0f-cac4-554d415e1dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z = 4.350\n",
            "ReLU(z) = 4.350\n",
            "Sigmoid(z) = 0.987\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def neuron_forward(x, w, b):\n",
        "    z = np.dot(w, x) + b\n",
        "    relu = np.maximum(0, z)\n",
        "    sigmoid = 1 / (1 + np.exp(-z))\n",
        "    return z, relu, sigmoid\n",
        "\n",
        "# Example\n",
        "x = np.array([2.0, -1.0, 3.0])\n",
        "w = np.array([0.5, -0.25, 1.0])\n",
        "b = 0.1\n",
        "\n",
        "z, relu_z, sigmoid_z = neuron_forward(x, w, b)\n",
        "print(f\"z = {z:.3f}\")\n",
        "print(f\"ReLU(z) = {relu_z:.3f}\")\n",
        "print(f\"Sigmoid(z) = {sigmoid_z:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2313705",
      "metadata": {
        "id": "f2313705"
      },
      "source": [
        "**Explanation:**  \n",
        "Activation functions are essential in neural networks because they introduce non-linearity, enabling the network to learn complex patterns. Without them, the network would behave like a linear model, regardless of its depth.  \n",
        "ReLU is typically used in hidden layers for its simplicity and effectiveness in mitigating vanishing gradients.  \n",
        "Sigmoid is used in output layers for binary classification, as it maps outputs to (0, 1), representing probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dd374a9",
      "metadata": {
        "id": "6dd374a9"
      },
      "source": [
        "### 1.2 Preprocessing + Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20aa34e",
      "metadata": {
        "id": "a20aa34e",
        "outputId": "ba2ad923-1f50-42b8-f4a5-1a09bef87d03"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'tabular.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1180366327.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tabular.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Columns:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tabular.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df = pd.read_csv('tabular.csv')\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns)\n",
        "print(\"Dtypes:\\n\", df.dtypes)\n",
        "print(\"Missing values:\\n\", df.isnull().sum())\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_cols = [col for col in categorical_cols if col not in ['customer_id']]\n",
        "\n",
        "for col in numeric_cols:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "for col in categorical_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "X = df.drop(['target', 'customer_id'], axis=1)\n",
        "y = df['target']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_cols)\n",
        "X_val = pd.get_dummies(X_val, columns=categorical_cols)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_cols)\n",
        "\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_val[numeric_cols] = scaler.transform(X_val[numeric_cols])\n",
        "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"Number of features after encoding:\", X_train.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09fbc188",
      "metadata": {
        "id": "09fbc188"
      },
      "source": [
        "**Explanation:**  \n",
        "Data leakage happens when information from outside the training set is used to create the model, leading to over-optimistic results. Fitting the scaler on the full dataset would leak information from validation/test into training, invalidating the evaluation. Always fit the scaler only on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abecd1a2",
      "metadata": {
        "id": "abecd1a2"
      },
      "source": [
        "### 1.3 FFNN Training + Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfe7fe29",
      "metadata": {
        "id": "dfe7fe29"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1, activation='linear')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Test MAE: {mae:.2f}\")\n",
        "print(f\"Test RMSE: {rmse:.2f}\")\n",
        "\n",
        "plt.scatter(y_test, y_pred, alpha=0.7)\n",
        "plt.xlabel('Actual Target')\n",
        "plt.ylabel('Predicted Target')\n",
        "plt.title('Parity Plot: Actual vs Predicted')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf747bc",
      "metadata": {
        "id": "baf747bc"
      },
      "source": [
        "### 1.4 Overfit/Underfit Diagnosis\n",
        "\n",
        "*Write your diagnosis and next steps here in markdown.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379ba871",
      "metadata": {
        "id": "379ba871"
      },
      "source": [
        "## Part 2 — NLP (Embeddings + RNN)\n",
        "### 2.1 Tokenization + Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a12ebd",
      "metadata": {
        "id": "f2a12ebd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "df_text = pd.read_csv('text.csv')\n",
        "print(df_text['label'].value_counts())\n",
        "\n",
        "df_text['text'] = df_text['text'].str.lower().str.strip().str.replace('\\s+', ' ', regex=True)\n",
        "\n",
        "X_text = df_text['text'].values\n",
        "y_text = df_text['label'].values\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_text, y_text, test_size=0.3, random_state=42, stratify=y_text)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "vocab_size = min(10000, len(tokenizer.word_index) + 1)\n",
        "print(\"Final vocabulary size:\", vocab_size)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_len = 40\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "print(\"Raw text:\", X_train[0])\n",
        "print(\"Token IDs:\", X_train_seq[0])\n",
        "print(\"Padded:\", X_train_pad[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "783e10fa",
      "metadata": {
        "id": "783e10fa"
      },
      "source": [
        "**Justification:**  \n",
        "A max_len of 40 covers most short feedback sentences, ensuring minimal truncation while keeping computation efficient.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719825f3",
      "metadata": {
        "id": "719825f3"
      },
      "source": [
        "### 2.2 Baseline Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a53b783",
      "metadata": {
        "id": "5a53b783"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "output_activation = 'sigmoid' if num_classes == 2 else 'softmax'\n",
        "loss_fn = 'binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy'\n",
        "\n",
        "model_nlp_base = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1 if num_classes == 2 else num_classes, activation=output_activation)\n",
        "])\n",
        "model_nlp_base.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "model_nlp_base.summary()\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model_nlp_base.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred = model_nlp_base.predict(X_test_pad)\n",
        "if num_classes == 2:\n",
        "    y_pred_label = (y_pred > 0.5).astype(int).flatten()\n",
        "else:\n",
        "    y_pred_label = np.argmax(y_pred, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred_label)\n",
        "f1 = f1_score(y_test, y_pred_label, average='weighted')\n",
        "cm = confusion_matrix(y_test, y_pred_label)\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "print(f\"Test F1-score: {f1:.3f}\")\n",
        "print(\"Confusion matrix:\\n\", cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48805b8c",
      "metadata": {
        "id": "48805b8c"
      },
      "source": [
        "### 2.3 RNN Model (SimpleRNN or LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca0baaf",
      "metadata": {
        "id": "9ca0baaf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
        "\n",
        "model_nlp_rnn = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "    LSTM(32),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1 if num_classes == 2 else num_classes, activation=output_activation)\n",
        "])\n",
        "model_nlp_rnn.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "model_nlp_rnn.summary()\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model_nlp_rnn.fit(\n",
        "    X_train_pad, y_train,\n",
        "    validation_data=(X_val_pad, y_val),\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    callbacks=[es],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred = model_nlp_rnn.predict(X_test_pad)\n",
        "if num_classes == 2:\n",
        "    y_pred_label = (y_pred > 0.5).astype(int).flatten()\n",
        "else:\n",
        "    y_pred_label = np.argmax(y_pred, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred_label)\n",
        "f1 = f1_score(y_test, y_pred_label, average='weighted')\n",
        "cm = confusion_matrix(y_test, y_pred_label)\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "print(f\"Test F1-score: {f1:.3f}\")\n",
        "print(\"Confusion matrix:\\n\", cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6baf171c",
      "metadata": {
        "id": "6baf171c"
      },
      "source": [
        "**Explanation:**  \n",
        "LSTM is chosen over SimpleRNN because it can capture longer dependencies and mitigate vanishing gradients, which is important for text data with context spread over several words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d860c1",
      "metadata": {
        "id": "51d860c1"
      },
      "source": [
        "### 2.4 Comparison + Transformer Note"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c80fc3b",
      "metadata": {
        "id": "0c80fc3b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "_ = model_nlp_base.fit(X_train_pad, y_train, epochs=1, batch_size=32, verbose=0)\n",
        "base_time = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "_ = model_nlp_rnn.fit(X_train_pad, y_train, epochs=1, batch_size=32, verbose=0)\n",
        "rnn_time = time.time() - start\n",
        "\n",
        "print(\"| Model      | Accuracy | F1-score | Training Time (1 epoch, s) |\")\n",
        "print(f\"| Baseline   | {acc:.3f}   | {f1:.3f}   | {base_time:.2f} |\")\n",
        "print(f\"| RNN/LSTM   | {acc:.3f}   | {f1:.3f}   | {rnn_time:.2f} |\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5659b6ea",
      "metadata": {
        "id": "5659b6ea"
      },
      "source": [
        "**Markdown Explanation:**  \n",
        "RNNs struggle with long sequences due to vanishing gradients and limited memory, making it hard to capture distant dependencies. Transformers address this with self-attention, allowing direct connections between all positions in the sequence, leading to better performance on long texts and parallelizable training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb815ff3",
      "metadata": {
        "id": "eb815ff3"
      },
      "source": [
        "## Part 3 — Computer Vision (CNN)\n",
        "### 3.1 Data Loading + Visual Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6577d70",
      "metadata": {
        "id": "d6577d70"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_size = (128, 128)\n",
        "batch_size = 32\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"images\",\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"images\",\n",
        "    validation_split=0.3,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Class names:\", class_names)\n",
        "for images, labels in train_ds.take(1):\n",
        "    print(\"Batch shape:\", images.shape, labels.shape)\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy())\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca91fdf6",
      "metadata": {
        "id": "ca91fdf6"
      },
      "source": [
        "### 3.2 CNN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60decac8",
      "metadata": {
        "id": "60decac8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model_cnn = models.Sequential([\n",
        "    layers.Input(shape=img_size + (3,)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
        "])\n",
        "model_cnn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model_cnn.summary()\n",
        "\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model_cnn.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    callbacks=[es]\n",
        ")\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('CNN Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c0d30d",
      "metadata": {
        "id": "e6c0d30d"
      },
      "source": [
        "### 3.3 Evaluation + Misclassifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b393ed",
      "metadata": {
        "id": "57b393ed"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "test_images, test_labels = next(iter(val_ds.unbatch().batch(len(val_ds))))\n",
        "y_pred = model_cnn.predict(test_images)\n",
        "if num_classes > 2:\n",
        "    y_pred_label = np.argmax(y_pred, axis=1)\n",
        "else:\n",
        "    y_pred_label = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "acc = accuracy_score(test_labels, y_pred_label)\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "\n",
        "cm = confusion_matrix(test_labels, y_pred_label)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"Classification report:\\n\", classification_report(test_labels, y_pred_label))\n",
        "\n",
        "mis_idx = np.where(test_labels != y_pred_label)[0][:5]\n",
        "plt.figure(figsize=(15, 3))\n",
        "for i, idx in enumerate(mis_idx):\n",
        "    ax = plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(test_images[idx].numpy())\n",
        "    plt.title(f\"True: {class_names[test_labels[idx]]}\\nPred: {class_names[y_pred_label[idx]]}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f0a7a2",
      "metadata": {
        "id": "50f0a7a2"
      },
      "source": [
        "**Markdown Write-up:**  \n",
        "Review the confusion matrix and misclassified images. Are errors concentrated in certain classes? Are there systematic confusions?  \n",
        "To improve, try data augmentation, a deeper CNN, or transfer learning with a pretrained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3911e00a",
      "metadata": {
        "id": "3911e00a"
      },
      "source": [
        "## Final Summary\n",
        "\n",
        "*Write 5–8 sentences summarizing your findings, model performance, and next steps.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}